{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPdpW0X6ugMjGSj/NvkBQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/multitrack-collector/gpt-4chan_notebook/blob/main/gpt_4chan_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See if you have a *Tesla T4* GPU by running command below. If you don't have one, you have to **change your runtime** under the runtime tab above."
      ],
      "metadata": {
        "id": "G6hYynDYsA4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvz8ditbu551",
        "outputId": "8f0ae279-33e9-4281-ffdb-5a41907cfc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 25 02:36:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inital Setup"
      ],
      "metadata": {
        "id": "d3UJKmalsNRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torrentp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yvYOSZFsHrJ",
        "outputId": "1ede35e8-f283-4ec8-a260-be7194391329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torrentp\n",
            "  Downloading torrentp-0.2.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting libtorrent>=2.0.7 (from torrentp)\n",
            "  Downloading libtorrent-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (249 bytes)\n",
            "Collecting asyncclick>=8.1.7.2 (from torrentp)\n",
            "  Downloading asyncclick-8.1.7.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from asyncclick>=8.1.7.2->torrentp) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio->asyncclick>=8.1.7.2->torrentp) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->asyncclick>=8.1.7.2->torrentp) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->asyncclick>=8.1.7.2->torrentp) (1.2.2)\n",
            "Downloading torrentp-0.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading asyncclick-8.1.7.2-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libtorrent-2.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libtorrent, asyncclick, torrentp\n",
            "Successfully installed asyncclick-8.1.7.2 libtorrent-2.0.9 torrentp-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# float16 (gpu)"
      ],
      "metadata": {
        "id": "t3TBPXyjsS2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.org/download/gpt4chan_model_float16/gpt4chan_model_float16_archive.torrent\n",
        "!wget https://github.com/Aspie96/gpt-4chan-model/raw/float16/config.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_99gtjHDveW1",
        "outputId": "97f1a606-7238-4327-e307-c8ce3c6ae2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-25 02:36:35--  https://archive.org/download/gpt4chan_model_float16/gpt4chan_model_float16_archive.torrent\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia902203.us.archive.org/16/items/gpt4chan_model_float16/gpt4chan_model_float16_archive.torrent [following]\n",
            "--2024-07-25 02:36:35--  https://ia902203.us.archive.org/16/items/gpt4chan_model_float16/gpt4chan_model_float16_archive.torrent\n",
            "Resolving ia902203.us.archive.org (ia902203.us.archive.org)... 207.241.228.63\n",
            "Connecting to ia902203.us.archive.org (ia902203.us.archive.org)|207.241.228.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59655 (58K) [application/x-bittorrent]\n",
            "Saving to: ‘gpt4chan_model_float16_archive.torrent’\n",
            "\n",
            "\r          gpt4chan_   0%[                    ]       0  --.-KB/s               \rgpt4chan_model_floa 100%[===================>]  58.26K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-25 02:36:35 (3.34 MB/s) - ‘gpt4chan_model_float16_archive.torrent’ saved [59655/59655]\n",
            "\n",
            "--2024-07-25 02:36:35--  https://github.com/Aspie96/gpt-4chan-model/raw/float16/config.json\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Aspie96/gpt-4chan-model/float16/config.json [following]\n",
            "--2024-07-25 02:36:36--  https://raw.githubusercontent.com/Aspie96/gpt-4chan-model/float16/config.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 836 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     836  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-25 02:36:36 (58.1 MB/s) - ‘config.json’ saved [836/836]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torrentp import TorrentDownloader\n",
        "torrent_file = TorrentDownloader(\"gpt4chan_model_float16_archive.torrent\", '.')\n",
        "await torrent_file.start_download()\n",
        "!cp config.json ./gpt4chan_model_float16/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhdnpojvs2Xo",
        "outputId": "ec43f249-af18-45e1-e70d-7aeb637f3001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95mSize: 12106074.81 MB\u001b[0m\n",
            "\u001b[95mSaving as: gpt4chan_model_float16\u001b[0m\n",
            "\u001b[42m 6022.4 Kb/s \u001b[0m|\u001b[46m up: 223.2 Kb/s \u001b[0m| status: seeding | peers: 0  \u001b[96m|####################|\u001b[0m 100%\u001b[92m\n",
            "Downloaded successfully.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "from transformers import GPTJForCausalLM\n",
        "import torch\n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\n",
        "    \"./gpt4chan_model_float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        ")\n",
        "model.cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "prompt = (\n",
        "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
        "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
        "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
        ")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.cuda()\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    max_length=100,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n"
      ],
      "metadata": {
        "id": "4Yd-fOu9x1ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#float32 (cpu)"
      ],
      "metadata": {
        "id": "6rEegtP8sVh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.org/download/gpt4chan_model/gpt4chan_model_archive.torrent\n",
        "!wget https://github.com/Aspie96/gpt-4chan-model/main/float16/config.json\n",
        "\n"
      ],
      "metadata": {
        "id": "EvyyPztXr8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torrentp import TorrentDownloader\n",
        "torrent_file = TorrentDownloader(\"gpt4chan_model_archive.torrent\", '.')\n",
        "await torrent_file.start_download()\n",
        "!cp config.json ./gpt4chan_model/"
      ],
      "metadata": {
        "id": "uVtal_rzvN2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt4chan_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "prompt = (\n",
        "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
        "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
        "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
        ")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    max_length=100,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
      ],
      "metadata": {
        "id": "Xul6C7vRr-IQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}